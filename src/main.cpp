#include "tokenizer.hpp"
#include "zipf_analyzer.hpp"
#include "mongodb_client.hpp"

#include <iostream>
#include <iomanip>
#include <chrono>
#include <fstream>

using namespace wiki;

void print_usage() {
    std::cout << "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:\n";
    std::cout << "  ./tokenizer <config.yaml>              - –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–µ—Å—å –∫–æ—Ä–ø—É—Å\n";
    std::cout << "  ./tokenizer <config.yaml> --limit 100  - –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å 100 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n";
    std::cout << "  ./tokenizer <config.yaml> --test       - —Ç–µ—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º (10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\n";
}

void print_statistics(const TokenizerStats& stats) {
    std::cout << "\n" << std::string(60, '=') << "\n";
    std::cout << "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò\n";
    std::cout << std::string(60, '=') << "\n";
    
    std::cout << "\nüìÅ –î–æ–∫—É–º–µ–Ω—Ç—ã:\n";
    std::cout << "   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: " << stats.total_documents << "\n";
    std::cout << "   –†–∞–∑–º–µ—Ä: " << std::fixed << std::setprecision(2) 
              << (stats.total_bytes / 1024.0 / 1024.0) << " –ú–ë\n";
    
    std::cout << "\nüìù –¢–æ–∫–µ–Ω—ã:\n";
    std::cout << "   –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: " << stats.total_tokens << "\n";
    std::cout << "   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: " << stats.unique_tokens << "\n";
    std::cout << "   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–µ–º–æ–≤: " << stats.unique_stems << "\n";
    std::cout << "   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: " << std::fixed << std::setprecision(2) 
              << stats.avg_token_length() << " —Å–∏–º–≤–æ–ª–æ–≤\n";
    
    std::cout << "\n‚è±Ô∏è –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:\n";
    std::cout << "   –í—Ä–µ–º—è: " << std::fixed << std::setprecision(2) 
              << stats.processing_time_sec << " —Å–µ–∫\n";
    std::cout << "   –°–∫–æ—Ä–æ—Å—Ç—å: " << std::fixed << std::setprecision(0) 
              << stats.tokens_per_second() << " —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫\n";
    std::cout << "   –°–∫–æ—Ä–æ—Å—Ç—å: " << std::fixed << std::setprecision(2) 
              << stats.kb_per_second() << " –ö–ë/—Å–µ–∫\n";
    
    // –¢–æ–ø-20 —Ç–æ–∫–µ–Ω–æ–≤
    std::vector<std::pair<std::string, size_t>> sorted_tokens(
        stats.token_freq.begin(), stats.token_freq.end());
    std::sort(sorted_tokens.begin(), sorted_tokens.end(),
              [](const auto& a, const auto& b) { return a.second > b.second; });
    
    std::cout << "\nüîù –¢–æ–ø-20 —Ç–æ–∫–µ–Ω–æ–≤:\n";
    for (size_t i = 0; i < std::min(size_t(20), sorted_tokens.size()); ++i) {
        std::cout << "   " << std::setw(2) << (i + 1) << ". " 
                  << sorted_tokens[i].first << ": " << sorted_tokens[i].second << "\n";
    }
    
    // –¢–æ–ø-20 —Å—Ç–µ–º–æ–≤
    std::vector<std::pair<std::string, size_t>> sorted_stems(
        stats.stem_freq.begin(), stats.stem_freq.end());
    std::sort(sorted_stems.begin(), sorted_stems.end(),
              [](const auto& a, const auto& b) { return a.second > b.second; });
    
    std::cout << "\nüîù –¢–æ–ø-20 —Å—Ç–µ–º–æ–≤:\n";
    for (size_t i = 0; i < std::min(size_t(20), sorted_stems.size()); ++i) {
        std::cout << "   " << std::setw(2) << (i + 1) << ". " 
                  << sorted_stems[i].first << ": " << sorted_stems[i].second << "\n";
    }
    
    std::cout << std::string(60, '=') << "\n";
}

void save_statistics(const TokenizerStats& stats, const std::string& path) {
    std::ofstream file(path);
    if (!file.is_open()) {
        std::cerr << "–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ " << path << std::endl;
        return;
    }
    
    file << "–°–¢–ê–¢–ò–°–¢–ò–ö–ê –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò\n";
    file << std::string(60, '=') << "\n\n";
    
    file << "–î–û–ö–£–ú–ï–ù–¢–´:\n";
    file << "  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: " << stats.total_documents << "\n";
    file << "  –†–∞–∑–º–µ—Ä: " << (stats.total_bytes / 1024.0 / 1024.0) << " –ú–ë\n\n";
    
    file << "–¢–û–ö–ï–ù–´:\n";
    file << "  –í—Å–µ–≥–æ: " << stats.total_tokens << "\n";
    file << "  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤: " << stats.unique_tokens << "\n";
    file << "  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–µ–º–æ–≤: " << stats.unique_stems << "\n";
    file << "  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: " << stats.avg_token_length() << "\n\n";
    
    file << "–ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:\n";
    file << "  –í—Ä–µ–º—è: " << stats.processing_time_sec << " —Å–µ–∫\n";
    file << "  –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫: " << stats.tokens_per_second() << "\n";
    file << "  –ö–ë/—Å–µ–∫: " << stats.kb_per_second() << "\n\n";
    
    // –¢–æ–ø-100 —Ç–æ–∫–µ–Ω–æ–≤
    std::vector<std::pair<std::string, size_t>> sorted_tokens(
        stats.token_freq.begin(), stats.token_freq.end());
    std::sort(sorted_tokens.begin(), sorted_tokens.end(),
              [](const auto& a, const auto& b) { return a.second > b.second; });
    
    file << "–¢–û–ü-100 –¢–û–ö–ï–ù–û–í:\n";
    for (size_t i = 0; i < std::min(size_t(100), sorted_tokens.size()); ++i) {
        file << "  " << (i + 1) << ". " << sorted_tokens[i].first 
             << ": " << sorted_tokens[i].second << "\n";
    }
    
    file.close();
    std::cout << "üìÑ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: " << path << std::endl;
}

int main(int argc, char* argv[]) {
    if (argc < 2) {
        print_usage();
        return 1;
    }
    
    std::string config_path = argv[1];
    size_t limit = 0;
    
    // –ü–∞—Ä—Å–∏–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã
    for (int i = 2; i < argc; ++i) {
        std::string arg = argv[i];
        if (arg == "--limit" && i + 1 < argc) {
            limit = std::stoul(argv[++i]);
        } else if (arg == "--test") {
            limit = 10;
        }
    }
    
    std::cout << std::string(60, '=') << "\n";
    std::cout << "üî§ –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –ò –ê–ù–ê–õ–ò–ó –ö–û–†–ü–£–°–ê (C++)\n";
    std::cout << std::string(60, '=') << "\n";
    
    try {
        // –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        DbConfig db_config = load_config(config_path);
        
        // –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ MongoDB
        MongoDBClient db_client(db_config);
        if (!db_client.connect()) {
            return 1;
        }
        
        size_t total_docs = db_client.count_documents();
        if (limit > 0) {
            total_docs = std::min(total_docs, limit);
        }
        
        std::cout << "\nüìö –û–±—Ä–∞–±–æ—Ç–∫–∞ " << total_docs << " –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...\n";
        std::cout << std::string(60, '=') << "\n";
        
        // –°–æ–∑–¥–∞—ë–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        Tokenizer::Config tok_config;
        tok_config.min_length = 2;
        tok_config.remove_numbers = true;
        tok_config.remove_stopwords = true;
        tok_config.apply_stemming = true;
        
        Tokenizer tokenizer(tok_config);
        TokenizerStats stats;
        
        // –ó–∞—Å–µ–∫–∞–µ–º –≤—Ä–µ–º—è
        auto start_time = std::chrono::high_resolution_clock::now();
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        db_client.for_each_document([&](const Document& doc) {
            stats.total_documents++;
            
            if (doc.html_content.empty()) return;
            
            stats.total_bytes += doc.html_content.size();
            
            // –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ —Å—Ç–µ–º–º–∏–Ω–≥
            auto [tokens, stems] = tokenizer.process_html(doc.html_content);
            
            stats.total_tokens += tokens.size();
            
            for (const auto& token : tokens) {
                stats.token_freq[token]++;
            }
            for (const auto& stem : stems) {
                stats.stem_freq[stem]++;
            }
            
            // –ü—Ä–æ–≥—Ä–µ—Å—Å
            if (stats.total_documents % 100 == 0) {
                auto now = std::chrono::high_resolution_clock::now();
                double elapsed = std::chrono::duration<double>(now - start_time).count();
                double speed = stats.total_documents / elapsed;
                
                std::cout << "  [" << stats.total_documents << "/" << total_docs << "] "
                          << "—Ç–æ–∫–µ–Ω–æ–≤: " << stats.total_tokens << ", "
                          << "—Å–∫–æ—Ä–æ—Å—Ç—å: " << std::fixed << std::setprecision(1) 
                          << speed << " –¥–æ–∫/—Å–µ–∫\n";
            }
        }, limit);
        
        // –ó–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
        auto end_time = std::chrono::high_resolution_clock::now();
        stats.processing_time_sec = std::chrono::duration<double>(end_time - start_time).count();
        stats.unique_tokens = stats.token_freq.size();
        stats.unique_stems = stats.stem_freq.size();
        
        // –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print_statistics(stats);
        
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        save_statistics(stats, "tokenization_stats.txt");
        
        // –°—Ç—Ä–æ–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∑–∞–∫–æ–Ω–∞ –¶–∏–ø—Ñ–∞
        if (!stats.stem_freq.empty()) {
            std::cout << "\nüìà –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–∞ –¶–∏–ø—Ñ–∞...\n";
            
            // –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            ZipfAnalyzer::save_plot_data(stats.stem_freq, "zipf_data.tsv");
            
            // –°–æ—Ä—Ç–∏—Ä—É–µ–º —á–∞—Å—Ç–æ—Ç—ã
            std::vector<size_t> frequencies;
            for (const auto& [stem, count] : stats.stem_freq) {
                frequencies.push_back(count);
            }
            std::sort(frequencies.begin(), frequencies.end(), std::greater<>());
            
            // –ü–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ú–∞–Ω–¥–µ–ª—å–±—Ä–æ—Ç–∞
            auto params = ZipfAnalyzer::fit_mandelbrot(frequencies);
            
            std::cout << "\nüî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–∫–æ–Ω–∞ –ú–∞–Ω–¥–µ–ª—å–±—Ä–æ—Ç–∞:\n";
            std::cout << "   B (–ø–æ–∫–∞–∑–∞—Ç–µ–ª—å —Å—Ç–µ–ø–µ–Ω–∏) = " << std::fixed << std::setprecision(3) << params.B << "\n";
            std::cout << "   P (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç) = " << std::fixed << std::setprecision(3) << params.P << "\n";
            std::cout << "   œÅ (rho, —Å–¥–≤–∏–≥) = " << std::fixed << std::setprecision(3) << params.rho << "\n";
            
            // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∫—Ä–∏–ø—Ç gnuplot
            ZipfAnalyzer::generate_gnuplot_script(
                "zipf_data.tsv",
                "zipf_plot.png",
                "–ó–∞–∫–æ–Ω –¶–∏–ø—Ñ–∞ (—Å—Ç–µ–º—ã)",
                stats.total_tokens,
                params
            );
            
            // –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
            auto zipf_theoretical = ZipfAnalyzer::calculate_zipf(
                frequencies.size(), stats.total_tokens);
            ZipfAnalyzer::analyze_deviation(frequencies, zipf_theoretical);
        }
        
        std::cout << "\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\n";
        
    } catch (const std::exception& e) {
        std::cerr << "–û—à–∏–±–∫–∞: " << e.what() << std::endl;
        return 1;
    }
    
    return 0;
}